{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\hp\\anaconda3\\lib\\site-packages (3.5.1)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\hp\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Practice\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.csv('basic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|    _c0|_c1|       _c2|\n",
      "+-------+---+----------+\n",
      "|   NAME|AGE|EXPERIENCE|\n",
      "| Swapil| 27|         1|\n",
      "|Neelesh| 28|         2|\n",
      "|  Sukhi| 27|         3|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pyspark = spark.read.option('header','true').csv('basic_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   NAME|AGE|EXPERIENCE|\n",
      "+-------+---+----------+\n",
      "| Swapil| 27|         1|\n",
      "|Neelesh| 28|         2|\n",
      "|  Sukhi| 27|         3|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(NAME='Swapil', AGE='27', EXPERIENCE='1')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- AGE: string (nullable = true)\n",
      " |-- EXPERIENCE: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# READ DATASET 2\n",
    "df_pyspark = spark.read.option('header','true').csv('basic_data.csv',inferSchema = 'True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- NAME: string (nullable = true)\n",
      " |-- AGE: integer (nullable = true)\n",
      " |-- EXPERIENCE: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   NAME|AGE|EXPERIENCE|\n",
      "+-------+---+----------+\n",
      "| Swapil| 27|         1|\n",
      "|Neelesh| 28|         2|\n",
      "|  Sukhi| 27|         3|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark = spark.read.csv('basic_data.csv', header=True, inferSchema=True)\n",
    "df_pyspark.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(NAME='Swapil', AGE=27, EXPERIENCE=1),\n",
       " Row(NAME='Neelesh', AGE=28, EXPERIENCE=2),\n",
       " Row(NAME='Sukhi', AGE=27, EXPERIENCE=3)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_pyspark.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|   Name|Experience|\n",
      "+-------+----------+\n",
      "| Swapil|         1|\n",
      "|Neelesh|         2|\n",
      "|  Sukhi|         3|\n",
      "+-------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# selecting and showing columns\n",
    "df_pyspark.select(['Name','Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('NAME', 'string'), ('AGE', 'int'), ('EXPERIENCE', 'int')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data types\n",
    "df_pyspark.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+----------+\n",
      "|summary|   NAME|               AGE|EXPERIENCE|\n",
      "+-------+-------+------------------+----------+\n",
      "|  count|      3|                 3|         3|\n",
      "|   mean|   NULL|27.333333333333332|       2.0|\n",
      "| stddev|   NULL|0.5773502691896258|       1.0|\n",
      "|    min|Neelesh|                27|         1|\n",
      "|    max| Swapil|                28|         3|\n",
      "+-------+-------+------------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+------------------+----------+-----------------------+\n",
      "|summary|   NAME|               AGE|EXPERIENCE|Experience after 2 year|\n",
      "+-------+-------+------------------+----------+-----------------------+\n",
      "|  count|      3|                 3|         3|                      3|\n",
      "|   mean|   NULL|27.333333333333332|       2.0|                    4.0|\n",
      "| stddev|   NULL|0.5773502691896258|       1.0|                    1.0|\n",
      "|    min|Neelesh|                27|         1|                      3|\n",
      "|    max| Swapil|                28|         3|                      5|\n",
      "+-------+-------+------------------+----------+-----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# adding column in dataframe\n",
    "df_pyspark2 = df_pyspark.withColumn('Experience after 2 year', df_pyspark['Experience']+2)\n",
    "df_pyspark2.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   NAME|AGE|EXPERIENCE|\n",
      "+-------+---+----------+\n",
      "| Swapil| 27|         1|\n",
      "|Neelesh| 28|         2|\n",
      "|  Sukhi| 27|         3|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop column\n",
    "\n",
    "df_pyspark2 = df_pyspark2.drop('Experience after 2 year')\n",
    "df_pyspark2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---+----------+\n",
      "|New Name|AGE|EXPERIENCE|\n",
      "+--------+---+----------+\n",
      "|  Swapil| 27|         1|\n",
      "| Neelesh| 28|         2|\n",
      "|   Sukhi| 27|         3|\n",
      "+--------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# renaname columns \n",
    "df_pyspark2.withColumnRenamed('Name','New Name').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# handling missing values in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   NAME| AGE|EXPERIENCE|\n",
      "+-------+----+----------+\n",
      "| Swapil|  27|       1.0|\n",
      "|Neelesh|  28|       2.0|\n",
      "|  Sukhi|  27|       3.0|\n",
      "| Aditya|  26|      NULL|\n",
      "|   NULL|  28|       4.0|\n",
      "|   NULL|  35|       2.0|\n",
      "|  Rohit|NULL|       4.0|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark3 = spark.read.csv('basic_data.csv', header=True, inferSchema=True)\n",
    "df_pyspark3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+----------+\n",
      "|   NAME|AGE|EXPERIENCE|\n",
      "+-------+---+----------+\n",
      "| Swapil| 27|       1.0|\n",
      "|Neelesh| 28|       2.0|\n",
      "|  Sukhi| 27|       3.0|\n",
      "+-------+---+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# drop the rows \n",
    "\n",
    "# checking again \n",
    "\n",
    "df_pyspark3.na.drop().show()\n",
    "# df_pyspark3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   NAME| AGE|EXPERIENCE|\n",
      "+-------+----+----------+\n",
      "| Swapil|  27|       1.0|\n",
      "|Neelesh|  28|       2.0|\n",
      "|  Sukhi|  27|       3.0|\n",
      "| Aditya|  26|      NULL|\n",
      "|   NULL|  28|       4.0|\n",
      "|   NULL|  35|       2.0|\n",
      "|  Rohit|NULL|       4.0|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# full syntax for drop\n",
    "df_pyspark3.na.drop(how=\"any\", thresh=2).show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   NAME| AGE|EXPERIENCE|\n",
      "+-------+----+----------+\n",
      "| Swapil|  27|       1.0|\n",
      "|Neelesh|  28|       2.0|\n",
      "|  Sukhi|  27|       3.0|\n",
      "|   NULL|  28|       4.0|\n",
      "|   NULL|  35|       2.0|\n",
      "|  Rohit|NULL|       4.0|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# subset\n",
    "df_pyspark3.na.drop(how='any', subset=['Experience']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+----+----------+\n",
      "|          NAME| AGE|EXPERIENCE|\n",
      "+--------------+----+----------+\n",
      "|        Swapil|  27|       1.0|\n",
      "|       Neelesh|  28|       2.0|\n",
      "|         Sukhi|  27|       3.0|\n",
      "|        Aditya|  26|      NULL|\n",
      "|Missing Values|  28|       4.0|\n",
      "|Missing Values|  35|       2.0|\n",
      "|         Rohit|NULL|       4.0|\n",
      "+--------------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill the missing value\n",
    "df_pyspark3.na.fill('Missing Values',['NAME', 'Age']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+\n",
      "|   NAME| AGE|EXPERIENCE|\n",
      "+-------+----+----------+\n",
      "| Swapil|  27|       1.0|\n",
      "|Neelesh|  28|       2.0|\n",
      "|  Sukhi|  27|       3.0|\n",
      "| Aditya|  26|      NULL|\n",
      "|   NULL|  28|       4.0|\n",
      "|   NULL|  35|       2.0|\n",
      "|  Rohit|NULL|       4.0|\n",
      "+-------+----+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pyspark3.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Imputer\n",
    "\n",
    "imputer = Imputer(\n",
    "        inputCols=['AGE', 'EXPERIENCE'],\n",
    "        outputCols=[\"{}_imputed\".format(c) for c in ['AGE', 'EXPERIENCE']]\n",
    "                   ).setStrategy(\"median\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+----------+-----------+------------------+\n",
      "|   NAME| AGE|EXPERIENCE|AGE_imputed|EXPERIENCE_imputed|\n",
      "+-------+----+----------+-----------+------------------+\n",
      "| Swapil|  27|       1.0|         27|               1.0|\n",
      "|Neelesh|  28|       2.0|         28|               2.0|\n",
      "|  Sukhi|  27|       3.0|         27|               3.0|\n",
      "| Aditya|  26|      NULL|         26|               2.0|\n",
      "|   NULL|  28|       4.0|         28|               4.0|\n",
      "|   NULL|  35|       2.0|         35|               2.0|\n",
      "|  Rohit|NULL|       4.0|         27|               4.0|\n",
      "+-------+----+----------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "imputer.fit(df_pyspark3).transform(df_pyspark3).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
